{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P27e_9MTWke0",
        "outputId": "65e5ba65-2f9f-4944-eb08-72d5b04551a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/rahuls/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMIS6xvFRgvA"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nhi4CB-hV_qV"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UJtXwYF9WO_6"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Hellow welcome to the world of NLP.\n",
        "This is the first step toward's learning NLP.\n",
        "we will be learning NLP in depth.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGX0kiQAWYYR",
        "outputId": "59cebe8d-3634-4d2e-c2c9-8403def0c827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hellow welcome to the world of NLP.', \"This is the first step toward's learning NLP.\", 'we will be learning NLP in depth.']\n"
          ]
        }
      ],
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqSlr5zRWemu",
        "outputId": "5a1d6413-1b84-4245-c1c7-5bbed6f27ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hellow', 'welcome', 'to', 'the', 'world', 'of', 'NLP', '.']\n",
            "['This', 'is', 'the', 'first', 'step', 'toward', \"'s\", 'learning', 'NLP', '.']\n",
            "['we', 'will', 'be', 'learning', 'NLP', 'in', 'depth', '.']\n"
          ]
        }
      ],
      "source": [
        "for doc in documents:\n",
        "  print(word_tokenize(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XkH9bejEW5_a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7HOTPqV0RG",
        "outputId": "13547a91-12b9-4275-869a-ca4adf9b323e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hellow',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'toward',\n",
              " \"'s\",\n",
              " 'learning',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'be',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'depth',\n",
              " '.']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oSHjL7mMV21k"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KF0sUnWXE6Z",
        "outputId": "c58d9b41-f4a2-4dcf-f4ce-5ab496b0cf2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hellow',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'toward',\n",
              " \"'\",\n",
              " 's',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'be',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'depth',\n",
              " '.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0vX1qNkgXHkc"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nDfHzg8NXV3l"
      },
      "outputs": [],
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxzCyOZiXazN",
        "outputId": "79c89a5b-e323-4f49-9661-5773097b79cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hellow',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " 'NLP.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'toward',\n",
              " \"'s\",\n",
              " 'learning',\n",
              " 'NLP.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'be',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'depth',\n",
              " '.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sDxtNnwEXfwL"
      },
      "outputs": [],
      "source": [
        "# steming\n",
        "words = [\"eating\",\"eaten\",\"eats\",\"writing\",\"writes\",\"programing\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptNRY4ZwRNZR"
      },
      "source": [
        "#### PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cQxwuDI6REpO"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c54hTasXRQEz",
        "outputId": "bd6e054b-702c-452d-ac90-a98475f7691c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "eats---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programing---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+stemming.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QOlpiKrdRY2y",
        "outputId": "ea0c314b-b2f4-4647-cf48-c91db10bfc51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'congratul'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemming.stem(\"congratulations\") # not good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PLG_TOPVSOk2",
        "outputId": "fb9c333e-93da-4309-b010-7516e6e17c0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sit'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemming.stem(\"sitting\") #good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAm6RMr6SXW0"
      },
      "source": [
        "#### RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5gNO33hMSWm6"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FUcStHD2Sf1l"
      },
      "outputs": [],
      "source": [
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BVGDn5fVS-oZ",
        "outputId": "19d5e4d3-3919-4cfc-ef82-a24d20e8991b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'eat'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_stemmer.stem(\"eating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GWur9HzrTB0x",
        "outputId": "7f53e471-c8c5-4ee4-f51c-20e78b236dfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ingeat'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_stemmer.stem(\"ingeating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFIeXlaVLXj"
      },
      "source": [
        "#### SnowballStemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2EN6yUpQTDt1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RlQ6-9Q7VUqH"
      },
      "outputs": [],
      "source": [
        "snowball = SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gdnCFEuuVcYM",
        "outputId": "c0b3ca7a-e342-4513-ebb8-d0461771965f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'histori'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowball.stem(\"history\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULSczZyqV8C3"
      },
      "source": [
        "snowball stemmer perform well compared to porter stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6msRkd84Vf1C",
        "outputId": "f1086674-f24b-4dae-e7af-141ed0ef8be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: Fairly == Porterstemmer -> fairli. snowballstemmer -> fair\n"
          ]
        }
      ],
      "source": [
        "print(\"word: Fairly == \"+\"Porterstemmer -> \"+stemming.stem(\"fairly\")+\". snowballstemmer -> \"+snowball.stem(\"fairly\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJIiZQKuYzXL"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "#### wordnetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Kf2Ad6F_VyzU"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PptfpRW0ZFxI"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l08z_whZQVW",
        "outputId": "b74fe2bb-166b-47f9-b619-fe7d0245a2aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/rahuls/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lizXQ7WvZKAO",
        "outputId": "cddb9c0a-5e47-46cb-f854-7c88a56dc378"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "POS:\n",
        "Noun - n\n",
        "Verb - v\n",
        "Adjective - a\n",
        "Adverb - r\n",
        "'''\n",
        "\n",
        "lemmatizer.lemmatize(\"going\",pos=\"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9muh4AnZNCr",
        "outputId": "c578b94f-d374-4d7c-b7dc-aa18413ab678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating---->eat\n",
            "eaten---->eat\n",
            "eats---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programing---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word,pos=\"v\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sW35FH8BZwWz",
        "outputId": "b595095a-e107-4120-f4bf-0fcb20cbc601"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"goes\",pos=\"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yQXvNYG8aE58",
        "outputId": "a9dd2117-902a-4519-cd1b-eb3230dbb519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'fairly'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos=\"v\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw789FYb1cyN"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aF1Bp5lYaITC"
      },
      "outputs": [],
      "source": [
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh5iMVNE2sK3",
        "outputId": "4cbd4d03-fffb-4a89-a73b-9b767e29e605"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/rahuls/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MlyDEgU02HRE"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvr2Kp4X2WD2",
        "outputId": "6f071cb0-50d5-4f1c-b1a2-081d5dc1797d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words(\"english\")\n",
        "#we can create our own stop words which is best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-v9GftM53bk8"
      },
      "outputs": [],
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6oVJPl63h0J",
        "outputId": "8bfa0598-cd77-4ddd-976c-68f5259cb697"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FWfH62H54Rvj"
      },
      "outputs": [],
      "source": [
        "## apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words =[stemming.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "  sentences[i] = ' '.join(words) # converting all the words into sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZNMFsHt5aGg",
        "outputId": "b957b2cd-f8f9-46a3-fa80-5ac0120c4957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us .',\n",
              " 'my second vision india ’ develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .',\n",
              " 'i see four mileston career']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fxnFUUMP6Arv"
      },
      "outputs": [],
      "source": [
        "## apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words =[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "  sentences[i] = ' '.join(words) # converting all the words into sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_IV8H3d8rpq",
        "outputId": "58e364a0-74b5-4020-9fa5-7e623ca3dfdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugues , british , french , dutch , come loot us , take .',\n",
              " 'yet do nation .',\n",
              " 'conquer anyon .',\n",
              " 'grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'believ india get first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nation world term gdp .',\n",
              " '10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'lucki work three close consid great opportun life .',\n",
              " 'see four mileston career']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WTSyhuEzq-c"
      },
      "source": [
        "### Parts of speech tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "qyXplXJh8tNb",
        "outputId": "530cc736-ab64-462c-add1-3f5db42de8c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nCC coordinating conjunction\\nCD cardinal digit\\nDT determiner\\nEX existential there (like: “there is” … think of it like “there exists”)\\nFW foreign word\\nIN preposition/subordinating conjunction\\nJJ adjective – ‘big’\\nJJR adjective, comparative – ‘bigger’\\nJJS adjective, superlative – ‘biggest’\\nLS list marker 1)\\nMD modal – could, will\\nNN noun, singular ‘- desk’\\nNNS noun plural – ‘desks’\\nNNP proper noun, singular – ‘Harrison’\\nNNPS proper noun, plural – ‘Americans’\\nPDT predeterminer – ‘all the kids’\\nPOS possessive ending parent’s\\nPRP personal pronoun –  I, he, she\\nPRP$ possessive pronoun – my, his, hers\\nRB adverb – very, silently,\\nRBR adverb, comparative – better\\nRBS adverb, superlative – best\\nRP particle – give up\\nTO – to go ‘to’ the store.\\nUH interjection – errrrrrrrm\\nVB verb, base form – take\\nVBD verb, past tense – took\\nVBG verb, gerund/present participle – taking\\nVBN verb, past participle – taken\\nVBP verb, sing. present, non-3d – take\\nVBZ verb, 3rd person sing. present – takes\\nWDT wh-determiner – which\\nWP wh-pronoun – who, what\\nWP$ possessive wh-pronoun, eg- whose\\nWRB wh-adverb, eg- where, when\\n'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "CC coordinating conjunction\n",
        "CD cardinal digit\n",
        "DT determiner\n",
        "EX existential there (like: “there is” … think of it like “there exists”)\n",
        "FW foreign word\n",
        "IN preposition/subordinating conjunction\n",
        "JJ adjective – ‘big’\n",
        "JJR adjective, comparative – ‘bigger’\n",
        "JJS adjective, superlative – ‘biggest’\n",
        "LS list marker 1)\n",
        "MD modal – could, will\n",
        "NN noun, singular ‘- desk’\n",
        "NNS noun plural – ‘desks’\n",
        "NNP proper noun, singular – ‘Harrison’\n",
        "NNPS proper noun, plural – ‘Americans’\n",
        "PDT predeterminer – ‘all the kids’\n",
        "POS possessive ending parent’s\n",
        "PRP personal pronoun –  I, he, she\n",
        "PRP$ possessive pronoun – my, his, hers\n",
        "RB adverb – very, silently,\n",
        "RBR adverb, comparative – better\n",
        "RBS adverb, superlative – best\n",
        "RP particle – give up\n",
        "TO – to go ‘to’ the store.\n",
        "UH interjection – errrrrrrrm\n",
        "VB verb, base form – take\n",
        "VBD verb, past tense – took\n",
        "VBG verb, gerund/present participle – taking\n",
        "VBN verb, past participle – taken\n",
        "VBP verb, sing. present, non-3d – take\n",
        "VBZ verb, 3rd person sing. present – takes\n",
        "WDT wh-determiner – which\n",
        "WP wh-pronoun – who, what\n",
        "WP$ possessive wh-pronoun, eg- whose\n",
        "WRB wh-adverb, eg- where, when\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qCTQxUdXq2rF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-1ChEyerRJU",
        "outputId": "e834a2d2-f81b-4861-cd42-4c4a9dee733c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my\\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of\\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India\\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be\\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of\\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OoUl-9ms7vF",
        "outputId": "1da44746-3922-453f-9b5b-2598959addaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/rahuls/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifcy_15QrWm_",
        "outputId": "f37a4503-fc64-4eba-fbbe-6ab16e94ba83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
            "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
            "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "### We will find the type of POS tag\n",
        "from nltk.corpus import stopwords\n",
        "for i in range(len(sentences)):\n",
        "  words= nltk.word_tokenize(sentences[i])\n",
        "  words= [word for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "  pos_tag = nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmBzN_gps5H6",
        "outputId": "5912b392-2bd8-409c-b0e6-4a984356be39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Beautiful', 'JJ'), ('Monument', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "print(nltk.pos_tag(\"Taj Mahal is a Beautiful Monument\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMPVCj7_2hVq"
      },
      "source": [
        "### Named entity recoginition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "rbAuFEEHuQWW",
        "outputId": "62188022-777b-4f09-86e5-eba170cf7884"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPerson Eg: Krish C Naik\\nPlace Or Location Eg: India\\nDate Eg: September,24-09-1989\\nTime  Eg: 4:30pm\\nMoney Eg: 1 million dollar\\nOrganization Eg: iNeuron Private Limited\\nPercent Eg: 20%, twenty percent\\n'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
        "\"\"\"\n",
        "Person Eg: Krish C Naik\n",
        "Place Or Location Eg: India\n",
        "Date Eg: September,24-09-1989\n",
        "Time  Eg: 4:30pm\n",
        "Money Eg: 1 million dollar\n",
        "Organization Eg: iNeuron Private Limited\n",
        "Percent Eg: 20%, twenty percent\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rovWcDqU2xe_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "words = nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ciDtc5rP3HHR"
      },
      "outputs": [],
      "source": [
        "tag_elements = nltk.pos_tag(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4g-J4ze3WWs",
        "outputId": "990083d8-224a-401e-be40-c1af7277799d"
      },
      "outputs": [],
      "source": [
        "# nltk.download('maxent_ne_chunker')\n",
        "# !pip install svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "qady00jm3Lb9",
        "outputId": "1e6713e3-c46f-463b-96ee-20781f106fbc"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1424.0,168.0\" width=\"1424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.80899%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.40449%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.42697%\" x=\"2.80899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.02247%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"11.236%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.6404%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"14.0449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.0112%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"17.9775%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.6629%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"21.3483%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0337%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"24.7191%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.8427%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"26.9663%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.6517%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"30.3371%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.4607%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.49438%\" x=\"32.5843%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">French</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8315%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"37.0787%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">engineer</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.8876%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.55056%\" x=\"42.6966%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.4719%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"52.2472%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0899%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"53.9326%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.8989%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.05618%\" x=\"57.8652%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.3933%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.30337%\" x=\"62.9213%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.573%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"70.2247%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.3483%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"72.4719%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.2809%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"78.0899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.0562%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"82.0225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.3933%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"88.764%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.1685%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"91.573%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.9438%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"98.3146%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.1573%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
            ],
            "text/plain": [
              "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('GPE', [('French', 'JJ')]), ('engineer', 'NN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.ne_chunk(tag_elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aQs5Mq8f3SzQ"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kv-e-cTG65Mm"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "vec_king = wv['king']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_king.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv['cricket'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cricketing', 0.8372224569320679),\n",
              " ('cricketers', 0.8165745139122009),\n",
              " ('Test_cricket', 0.8094819188117981),\n",
              " ('Twenty##_cricket', 0.8068488240242004),\n",
              " ('Twenty##', 0.7624265551567078),\n",
              " ('Cricket', 0.75413978099823),\n",
              " ('cricketer', 0.7372578382492065),\n",
              " ('twenty##', 0.7316356897354126),\n",
              " ('T##_cricket', 0.7304614186286926),\n",
              " ('West_Indies_cricket', 0.6987985968589783)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar('cricket')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('glad', 0.7408890724182129),\n",
              " ('pleased', 0.6632170677185059),\n",
              " ('ecstatic', 0.6626912355422974),\n",
              " ('overjoyed', 0.6599286794662476),\n",
              " ('thrilled', 0.6514049172401428),\n",
              " ('satisfied', 0.6437949538230896),\n",
              " ('proud', 0.636042058467865),\n",
              " ('delighted', 0.627237856388092),\n",
              " ('disappointed', 0.6269949674606323),\n",
              " ('excited', 0.6247665286064148)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar('happy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6510956"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.similarity('king','queen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "vec = wv['king']-wv['man']+wv['woman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300516366958618),\n",
              " ('monarch', 0.6454660296440125),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676948547363),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376776456832886),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar([vec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
